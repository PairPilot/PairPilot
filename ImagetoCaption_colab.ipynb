{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImagetoCaption.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN+rklYYg6mizdOcgXCXs4p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/forgallaxytabfgt/MachineLearning/blob/master/ImagetoCaption_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI5taKCtdbA1"
      },
      "source": [
        "# 필요한 것들 한번에 임포트해오기\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from numpy import array\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "import string\r\n",
        "import os\r\n",
        "from PIL import Image\r\n",
        "import glob\r\n",
        "from pickle import dump, load\r\n",
        "from time import time\r\n",
        "import keras\r\n",
        "from keras.preprocessing import sequence\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\r\n",
        "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\r\n",
        "from keras.optimizers import Adam, RMSprop\r\n",
        "from keras.layers.wrappers import Bidirectional\r\n",
        "from keras.layers.merge import add\r\n",
        "from keras.applications.inception_v3 import InceptionV3\r\n",
        "from keras.preprocessing import image\r\n",
        "from keras.models import Model\r\n",
        "from keras import Input, layers\r\n",
        "from keras import optimizers\r\n",
        "from keras.applications.inception_v3 import preprocess_input\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_xxJoIKdnWF",
        "outputId": "a3225483-0c92-4521-b88b-8d25733cd7c9"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/gdrive', force_remount=True)\r\n",
        "\r\n",
        "#train caption 파일 불러오기\r\n",
        "# 예제 코드 -> 대부분 함수로 선언해서 값 return 받는 형태\r\n",
        "def load_doc(filename):\r\n",
        "    file = open(filename, 'r')\r\n",
        "    text = file.read()\r\n",
        "    file.close()\r\n",
        "    return text\r\n",
        "\r\n",
        "filename = \"/gdrive/My Drive/Colab Notebooks/Mydata/train_captions.txt\"\r\n",
        "\r\n",
        "doc = load_doc(filename)\r\n",
        "print(doc[:300])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\n",
            "1000268201_693b08cb0e.jpg,A girl going into a wooden building .\n",
            "1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .\n",
            "1000268201_693b08cb0e.jpg,A little girl climbing the stairs to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0r_co5_dTa7"
      },
      "source": [
        "Now preprocessing again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BQ7b8ReizQM",
        "outputId": "8afbb87b-65fa-4cf3-d662-dc342f5d2a1b"
      },
      "source": [
        "def load_descriptions(doc):\r\n",
        "    mapping = dict() #train 데이터를 딕셔너리로\r\n",
        "    # 문장(행) 처리단계\r\n",
        "    for line in doc.split('\\n'):  # 라인 개행 -> 스페이스 바로 구분\r\n",
        "            tokens = line.split()\r\n",
        "            if len(line) < 2:\r\n",
        "                    continue\r\n",
        "        # 첫번째 토큰이 image id이면 -> 그 뒷부분은 캡션\r\n",
        "            image_id, image_desc = tokens[0], tokens[1:]\r\n",
        "        # image id에서 파일 네임 추출\r\n",
        "            image_id = image_id.split('.')[0]\r\n",
        "            image_desc = ' '.join(image_desc)\r\n",
        "\r\n",
        "            if image_id not in mapping:\r\n",
        "                    mapping[image_id] = list()\r\n",
        "            \r\n",
        "        # description 저장\r\n",
        "            mapping[image_id].append(image_desc)\r\n",
        "    return mapping\r\n",
        "\r\n",
        "# 함수 리턴값을 변수descriptions 에 저장\r\n",
        "descriptions = load_descriptions(doc)\r\n",
        "print('불러온 descriptions: %d ' % len(descriptions))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "불러온 descriptions: 6000 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G4uc5kSMyS-",
        "outputId": "4c15974c-faa7-44ad-e541-bf1e02ff1d67"
      },
      "source": [
        "list(descriptions.keys())[:5]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1000268201_693b08cb0e',\n",
              " '1001773457_577c3a7d70',\n",
              " '1002674143_1b742ab4b8',\n",
              " '1003163366_44323f5815',\n",
              " '1007129816_e794419615']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQb3O2BOM1wm",
        "outputId": "ce2328d1-9496-4b3e-c8c2-b2dfea165f5c"
      },
      "source": [
        "descriptions['1000268201_693b08cb0e']"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['child in a pink dress is climbing up a set of stairs in an entry way .',\n",
              " 'girl going into a wooden building .',\n",
              " 'little girl climbing into a wooden playhouse .',\n",
              " 'little girl climbing the stairs to her playhouse .',\n",
              " 'little girl in a pink dress going into a wooden cabin .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1W_1FI7hDqV",
        "outputId": "435e7e6f-f014-49ab-81b1-288588c3ed24"
      },
      "source": [
        "#데이터 클리닝 함수 선언 (구두점 제거등,,)\r\n",
        "def clean_descriptions(descriptions):\r\n",
        "        # translation table 이용하여 cleaning 시행\r\n",
        "        table = str.maketrans('', '', string.punctuation)\r\n",
        "        for key, desc_list in descriptions.items():\r\n",
        "            for i in range(len(desc_list)):\r\n",
        "                desc = desc_list[i] # 리스트화\r\n",
        "                desc = desc.split()\r\n",
        "                desc = [word.lower() for word in desc] # 모든 단어를 소문자로\r\n",
        "                # 구두점 제거\r\n",
        "                desc = [w.translate(table) for w in desc]\r\n",
        "                # hanging 's' and 'a' 제거 -> ??\r\n",
        "                desc = [word for word in desc if len(word)>1]\r\n",
        "                # remove tokens with numbers in them \r\n",
        "                desc = [word for word in desc if word.isalpha()]\r\n",
        "                # 문자열 형태로 저장\r\n",
        "                desc_list[i] =  ' '.join(desc)\r\n",
        "\r\n",
        "# 정의한 함수로 description clean\r\n",
        "clean_descriptions(descriptions)\r\n",
        "\r\n",
        "# clean 된 caption 예시\r\n",
        "descriptions['1000268201_693b08cb0e']"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['child in pink dress is climbing up set of stairs in an entry way',\n",
              " 'girl going into wooden building',\n",
              " 'little girl climbing into wooden playhouse',\n",
              " 'little girl climbing the stairs to her playhouse',\n",
              " 'little girl in pink dress going into wooden cabin']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0utXgEGM4AG",
        "outputId": "f890fb26-cfd2-4091-930f-c15bdbb98f73"
      },
      "source": [
        "\r\n",
        "# clean된 description 바탕으로 단어 사전 생성하는 함수 정의\r\n",
        "\r\n",
        "def to_vocabulary(descriptions):\r\n",
        "        # clean된 description -> list\r\n",
        "        all_desc = set()\r\n",
        "        for key in descriptions.keys():\r\n",
        "            [all_desc.update(d.split()) for d in descriptions[key]]\r\n",
        "        return all_desc\r\n",
        "\r\n",
        "vocabulary = to_vocabulary(descriptions)\r\n",
        "print('Original Vocabulary Size: %d' % len(vocabulary))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Vocabulary Size: 7600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrQIuO6MekC1"
      },
      "source": [
        "# 파일에 description 저장\r\n",
        "def save_descriptions(descriptions, filename):\r\n",
        "        lines = list()\r\n",
        "        for key, desc_list in descriptions.items():\r\n",
        "            for desc in desc_list:\r\n",
        "                lines.append(key + ' ' + desc)\r\n",
        "        data = '\\n'.join(lines)\r\n",
        "        file = open(filename, 'w')\r\n",
        "        file.write(data)\r\n",
        "        file.close()\r\n",
        "\r\n",
        "save_descriptions(descriptions, 'descriptions.txt')"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCXYqMSyemKb",
        "outputId": "d2909736-79b4-48f9-9a4f-8434ca095472"
      },
      "source": [
        "# photo identifiers 리스트 호출\r\n",
        "def load_set(filename):\r\n",
        "        doc = load_doc(filename)\r\n",
        "        dataset = list()\r\n",
        "        for line in doc.split('\\n'):\r\n",
        "            # 비어있는 라인 skip\r\n",
        "            if len(line) < 1:\r\n",
        "                continue\r\n",
        "\r\n",
        "            identifier = line.split('.')[0]\r\n",
        "            dataset.append(identifier)\r\n",
        "        return set(dataset)\r\n",
        "\r\n",
        "# train 데이터 불러오기\r\n",
        "filename = \"/gdrive/My Drive/Colab Notebooks/Mydata/train_captions.txt\"\r\n",
        "train = load_set(filename)\r\n",
        "print('Dataset: %d' % len(train))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13Mt8zPxQDbW"
      },
      "source": [
        "images = \"/gdrive/My Drive/Colab Notebooks/Mydata/Images/\"\r\n",
        "# 모든이미지 load\r\n",
        "img = glob.glob(images + '*.jpg')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h8E5KtFdnV0"
      },
      "source": [
        "#train에 사용할 data 리스트 생성\r\n",
        "\r\n",
        "train_images_file = \"/gdrive/My Drive/Colab Notebooks/Mydata/train_captions.txt\"\r\n",
        "# train image 이름 추출\r\n",
        "train_images = set(open(train_images_file, 'r').read().strip().split('\\n'))\r\n",
        "\r\n",
        "# train 이미지 리스트 생성\r\n",
        "train_img = []\r\n",
        "for i in img: \r\n",
        "    if i[len(images):] in train_images:\r\n",
        "        train_img.append(i) "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GYW81Ced4s7"
      },
      "source": [
        "#test에 사용할 data 리스트 생성\r\n",
        "test_images_file = \"/gdrive/My Drive/Colab Notebooks/Mydata/test_captions.txt\"\r\n",
        "# test image 이름 추출\r\n",
        "test_images = set(open(test_images_file, 'r').read().strip().split('\\n'))\r\n",
        "\r\n",
        "# test 이미지 리스트 생성\r\n",
        "test_img = []\r\n",
        "for i in img: \r\n",
        "    if i[len(images):] in test_images: \r\n",
        "        test_img.append(i)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2EAk_3xeKzE",
        "outputId": "12a91ca9-18d6-4653-ac3d-a2aef2f42268"
      },
      "source": [
        "# cleaned description 메모리에 불러오기\r\n",
        "def load_clean_descriptions(filename, dataset):\r\n",
        "        doc = load_doc(filename)\r\n",
        "        descriptions = dict()\r\n",
        "        for line in doc.split('\\n'):\r\n",
        "            tokens = line.split()\r\n",
        "            image_id, image_desc = tokens[0], tokens[1:]\r\n",
        "            if image_id in dataset:\r\n",
        "                if image_id not in descriptions:\r\n",
        "                    descriptions[image_id] = list()\r\n",
        "                desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\r\n",
        "                descriptions[image_id].append(desc)\r\n",
        "        return descriptions\r\n",
        "\r\n",
        "#train descriptions 생성끝(clean 처리 완료) \r\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\r\n",
        "print('Descriptions: train=%d' % len(train_descriptions))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Descriptions: train=6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T1lDc1Uez6r"
      },
      "source": [
        "def preprocess(image_path):\r\n",
        "    # Inceptionv3 model 쓰기위해 이미지를 299x299 크기로 변환하는 단계\r\n",
        "    img = image.load_img(image_path, target_size=(299, 299))\r\n",
        "    # Inceptionv3 model 활용을 위해 PIL image -> 3-dimensions\r\n",
        "    x = image.img_to_array(img)\r\n",
        "    # 차원추가\r\n",
        "    x = np.expand_dims(x, axis=0)\r\n",
        "    # 이미지 전처리 - nception module에 있는 preprocess_input() 함수사용\r\n",
        "    x = preprocess_input(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "# InceptionV3 model 로드\r\n",
        "model = InceptionV3(weights='imagenet')\r\n",
        "# InceptionV3 model output layer 제거해서 새 모델 생성\r\n",
        "model_new = Model(model.input, model.layers[-2].output)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-phiXaPe1KC"
      },
      "source": [
        "\r\n",
        "# 이미지를 길이 2048짜리의 벡터로 변환하기 위한 encoding 함수 정의\r\n",
        "def encode(image):\r\n",
        "    image = preprocess(image) # preprocess the image\r\n",
        "    fea_vec = model_new.predict(image) # Get the encoding vector for the image\r\n",
        "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) # reshape from (1, 2048) to (2048, )\r\n",
        "    return fea_vec"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUaAvPnJfKST",
        "outputId": "df2a3b43-2b09-48f4-d714-a5f98abc125d"
      },
      "source": [
        "train_features = load(open(\"/gdrive/My Drive/Colab Notebooks/Mydata/encoded_train_images.pkl\", \"rb\"))\r\n",
        "print('Photos: train=%d' % len(train_features))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Photos: train=6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfJz0gk8fPLL",
        "outputId": "5dc89310-6b5f-4e97-9c8c-0a2f09bceb28"
      },
      "source": [
        "\r\n",
        "#train caption 리스트 생성\r\n",
        "all_train_captions = []\r\n",
        "for key, val in train_descriptions.items():\r\n",
        "    for cap in val:\r\n",
        "        all_train_captions.append(cap)\r\n",
        "len(all_train_captions)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSgAyx3EfQwD",
        "outputId": "aacdd0e3-4067-4f7c-9d20-0208cddc7d42"
      },
      "source": [
        "# 빈출 어휘만 추출\r\n",
        "# 코퍼스(말뭉치)에서 10번 이상 나타나는 단어만 뽑아내어 고려\r\n",
        "word_count_threshold = 10 # 10회로 지정\r\n",
        "word_counts = {}\r\n",
        "nsents = 0\r\n",
        "for sent in all_train_captions:\r\n",
        "    nsents += 1\r\n",
        "    for w in sent.split(' '):\r\n",
        "        word_counts[w] = word_counts.get(w, 0) + 1\r\n",
        "\r\n",
        "# 등장횟수 만족하는 word만 vocab에 저장\r\n",
        "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\r\n",
        "print('preprocessed words %d -> %d' % (len(word_counts), len(vocab)))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocessed words 7603 -> 1652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5T0CPZGfSzG",
        "outputId": "dc62edba-1687-4c6a-85b8-e5200a1c6bf9"
      },
      "source": [
        "\r\n",
        "#단어를 인덱스로 변환\r\n",
        "ixtoword = {}\r\n",
        "wordtoix = {}\r\n",
        "\r\n",
        "ix = 1\r\n",
        "for w in vocab:\r\n",
        "    wordtoix[w] = ix\r\n",
        "    ixtoword[ix] = w\r\n",
        "    ix += 1\r\n",
        "    \r\n",
        "vocab_size = len(ixtoword) + 1 # 추가된 0 때문에 1 더하기\r\n",
        "vocab_size"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1653"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK5zesEEfVTs",
        "outputId": "7d05bb22-6df5-42fc-d98d-48c32d3d44db"
      },
      "source": [
        "# 딕셔너리에서 리스트로 변환\r\n",
        "\r\n",
        "def to_lines(descriptions):\r\n",
        "\tall_desc = list()\r\n",
        "\tfor key in descriptions.keys():\r\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\r\n",
        "\treturn all_desc\r\n",
        "\r\n",
        "# 가장 많은 단어 사용한 캡션 길이\r\n",
        "def max_length(descriptions):\r\n",
        "\tlines = to_lines(descriptions)\r\n",
        "\treturn max(len(d.split()) for d in lines)\r\n",
        "\r\n",
        "# 최대 캡션 길이 정하기\r\n",
        "max_length = max_length(train_descriptions)\r\n",
        "print('Description Length: %d' % max_length)\r\n",
        "# 왜 34가 아니라 33으로 나오지..? train dataset을 잘못처리한건가"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Description Length: 33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYNruwhAfXtd"
      },
      "source": [
        "# GENERATOR !!!!!!!!\r\n",
        "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\r\n",
        "    X1, X2, y = list(), list(), list()\r\n",
        "    n=0\r\n",
        "    # loop for ever over images\r\n",
        "    while 1:\r\n",
        "        for key, desc_list in descriptions.items():\r\n",
        "            n+=1\r\n",
        "            # retrieve the photo feature\r\n",
        "            photo = photos[key+'.jpg']\r\n",
        "            for desc in desc_list:\r\n",
        "                # encode the sequence\r\n",
        "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\r\n",
        "                # split one sequence into multiple X, y pairs\r\n",
        "                for i in range(1, len(seq)):\r\n",
        "                    # input 시퀀스랑 output 시퀀스로 분리\r\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\r\n",
        "                    # 인풋시퀀스 제로 패딩해서 길이 max로 맞추기\r\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\r\n",
        "                    # 아웃풋 시퀀스 인코딩\r\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\r\n",
        "   \r\n",
        "                    X1.append(photo)\r\n",
        "                    X2.append(in_seq)\r\n",
        "                    y.append(out_seq)\r\n",
        "\r\n",
        "            if n==num_photos_per_batch:\r\n",
        "                yield [[array(X1), array(X2)], array(y)]\r\n",
        "                X1, X2, y = list(), list(), list()\r\n",
        "                n=0"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr-dfhOUfaZt",
        "outputId": "67df3a4c-f804-457c-eaa1-08aa413a2666"
      },
      "source": [
        "# Glove Embedding 단계\r\n",
        "\r\n",
        "# Load Glove vectors\r\n",
        "glove_dir = '/gdrive/My Drive/Colab Notebooks/Mydata/'\r\n",
        "embeddings_index = {} \r\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\r\n",
        "\r\n",
        "for line in f:\r\n",
        "    values = line.split()\r\n",
        "    word = values[0]\r\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\r\n",
        "    embeddings_index[word] = coefs\r\n",
        "f.close()\r\n",
        "print('Found %s word vectors.' % len(embeddings_index))\r\n",
        "\r\n",
        "embedding_dim = 200\r\n",
        "\r\n",
        "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\r\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\r\n",
        "\r\n",
        "for word, i in wordtoix.items():\r\n",
        "    #if i < max_words:\r\n",
        "    embedding_vector = embeddings_index.get(word)\r\n",
        "    if embedding_vector is not None:\r\n",
        "        # Words not found in the embedding index will be all zeros\r\n",
        "        embedding_matrix[i] = embedding_vector\r\n",
        "        \r\n",
        "embedding_matrix.shape"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1653, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML95mcQJfhvU",
        "outputId": "66a3383c-8fdf-476e-d5d6-2617c11ce0a4"
      },
      "source": [
        "inputs1 = Input(shape=(2048,))\r\n",
        "fe1 = Dropout(0.5)(inputs1)\r\n",
        "fe2 = Dense(256, activation='relu')(fe1)\r\n",
        "inputs2 = Input(shape=(max_length,))\r\n",
        "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\r\n",
        "se2 = Dropout(0.5)(se1)\r\n",
        "se3 = LSTM(256)(se2)\r\n",
        "decoder1 = add([fe2, se3])\r\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\r\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\r\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 33)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 2048)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 33, 200)      330600      input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 2048)         0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 33, 200)      0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 256)          524544      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 256)          467968      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1653)         424821      dense_4[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,813,725\n",
            "Trainable params: 1,813,725\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        },
        "id": "2kiFF_w3fb93",
        "outputId": "274cbe8a-5621-488a-bcb9-79e114dd9228"
      },
      "source": [
        "### OS에러 발생\r\n",
        "\r\n",
        "model.layers[2]\r\n",
        "\r\n",
        "model.layers[2].set_weights([embedding_matrix])\r\n",
        "model.layers[2].trainable = False\r\n",
        "\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n",
        "\r\n",
        "epochs = 10\r\n",
        "number_pics_per_bath = 3\r\n",
        "steps = len(train_descriptions)//number_pics_per_bath\r\n",
        "\r\n",
        "for i in range(epochs):\r\n",
        "    generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_bath)\r\n",
        "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\r\n",
        "    model.save('/gdrive/My Drive/Colab Notebooks/Mydata' + str(i) + '.h5')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-b9b0fb414b0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_descriptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordtoix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_pics_per_bath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gdrive/My Drive/Colab Notebooks/Mydata'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:207 assert_input_compatibility\n        ' input tensors. Inputs received: ' + str(inputs))\n\n    ValueError: Layer model_4 expects 2 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, None) dtype=int32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, None) dtype=float32>]\n"
          ]
        }
      ]
    }
  ]
}